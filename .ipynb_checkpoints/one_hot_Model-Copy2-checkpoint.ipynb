{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fedcc002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e763c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prolific = 99\n",
    "n_text = 4999\n",
    "n_authors = 21245\n",
    "n_prolific = 100\n",
    "n_years = 19\n",
    "n_venues = 464"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5d366b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train json file\n",
    "train_filename = './data/train.json'\n",
    "with open(train_filename, 'r', encoding='utf-8') as f:\n",
    "    raw_train = json.load(f)\n",
    "# read test json file\n",
    "test_filename = './data/test.json'\n",
    "with open(test_filename, 'r', encoding='utf-8') as f:\n",
    "    raw_test = json.load(f)\n",
    "    \n",
    "# get a copy\n",
    "train = raw_train.copy()\n",
    "test = raw_test.copy()\n",
    "\n",
    "# extract coauthors as a new key from train.json\n",
    "for i in range(len(train)):\n",
    "    coauthors = []\n",
    "    prolific_authors = []\n",
    "    for auth in train[i]['authors']:\n",
    "        if auth >= max_prolific:\n",
    "            coauthors.append(auth)\n",
    "        else:\n",
    "            prolific_authors.append(auth)\n",
    "    train[i]['coauthors'] = coauthors\n",
    "    #if len(prolific_authors) == 0:\n",
    "        #prolific_authors.append(-1)\n",
    "    train[i]['prolific_authors'] = prolific_authors\n",
    "\n",
    "train_df = pd.DataFrame.from_dict(train)\n",
    "train_df = train_df.drop(['authors'], axis=1)\n",
    "test_df = pd.DataFrame.from_dict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a652a73",
   "metadata": {},
   "source": [
    "## Nerual Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4897397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6c9a1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 40\n",
    "hidden_dim = 200\n",
    "output_dim = 101\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0deae350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features_torch(df, have_prolific):\n",
    "    features = []\n",
    "    targets = []\n",
    "    for i in range(df.shape[0]):\n",
    "        # abstract and title\n",
    "        abstract_list = [0] * n_text\n",
    "        title_list = [0] * n_text\n",
    "        current_row = df.loc[i]\n",
    "        abstract_freq = collections.Counter(current_row['abstract'])\n",
    "        title_freq = collections.Counter(current_row['title'])\n",
    "        for key, value in dict(abstract_freq).items():\n",
    "            abstract_list[key-1] = value\n",
    "        for key, value in dict(title_freq).items():\n",
    "            title_list[key-1] = value\n",
    "        # year\n",
    "        year_list = [0] * n_years\n",
    "        year_list[current_row['year']-1] = 1\n",
    "        # venue\n",
    "        venue_list = [0] * (n_venues + 2) # 466 elements with the last element for empty venue\n",
    "        if current_row['venue'] == '':\n",
    "            venue_list[-1] = 1\n",
    "        else:\n",
    "            venue_list[current_row['venue']] = 1\n",
    "        # coauthors\n",
    "        coauthor_list = [0] * (n_authors - n_prolific + 2) # 21147 elements with the last element for empty coauthors\n",
    "        if current_row['coauthors'] == []:\n",
    "            coauthor_list[-1] = 1\n",
    "        else:\n",
    "            for coauthor in current_row['coauthors']:\n",
    "                coauthor_list[coauthor-n_prolific] = 1\n",
    "           \n",
    "        combined_features = title_list + coauthor_list\n",
    "        features.append(np.array(combined_features))\n",
    "        \n",
    "        if have_prolific == True:\n",
    "            # prolific authors\n",
    "            prolific_list = [0] * (n_prolific + 1) # 101 elements with the last element for empty coauthors\n",
    "            if current_row['prolific_authors'] == []:\n",
    "                prolific_list[-1] = 1\n",
    "            else:\n",
    "                for prolific in current_row['prolific_authors']:\n",
    "                    prolific_list[prolific] = 1\n",
    "            targets.append(np.array(prolific_list))\n",
    "    if have_prolific == True:\n",
    "        X_train = np.vstack(features)\n",
    "        y_train = np.array(targets)\n",
    "        return X_train, y_train\n",
    "    else:\n",
    "        X_test = np.vstack(features)\n",
    "        return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6177e64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuthorDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X, y, istrain):\n",
    "        self.X = X\n",
    "        self.istrain = istrain\n",
    "        if self.istrain == True:\n",
    "            self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.istrain == True:\n",
    "            return self.X[index], self.y[index]\n",
    "        else:\n",
    "            return self.X[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8af45b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = combine_features_torch(train_df, have_prolific=True)\n",
    "X_test = combine_features_torch(test_df, have_prolific=False)\n",
    "training_df = AuthorDataset(X_train, y_train, istrain = True)\n",
    "testing_df = AuthorDataset(X_test, y_train, istrain = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39d3fba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    # for training set\n",
    "    if len(batch[0]) == 2:\n",
    "        output = {\"x\": [], \"target\": []}\n",
    "        \n",
    "        for data in batch:\n",
    "            x = data[0]\n",
    "            target = data[1]\n",
    "            output['x'] += [x]\n",
    "            output['target'] += [target]\n",
    "            \n",
    "        output['x'] = torch.tensor(output['x'], dtype=torch.float)\n",
    "        output['target'] = torch.tensor(output['target'], dtype=torch.float)\n",
    "        return output\n",
    "    \n",
    "    # for testing set\n",
    "    else:\n",
    "        output = {\"x\": []}\n",
    "        for data in batch:\n",
    "            output['x'] += [data]\n",
    "        output['x'] = torch.tensor(output['x'], dtype=torch.float)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "793f91c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3h/x2xrdjwn22bbdw5d98hd13w80000gn/T/ipykernel_24878/626078167.py:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:204.)\n",
      "  output['x'] = torch.tensor(output['x'], dtype=torch.float)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'x': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'target': tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = DataLoader(dataset = training_df, batch_size = batch_size, shuffle=True, collate_fn = my_collate)\n",
    "#val_dataloader = DataLoader(dataset = validation_df, batch_size = batch_size, shuffle=False, collate_fn = my_collate)\n",
    "test_dataloader = DataLoader(dataset = testing_df, batch_size = batch_size, shuffle=False, collate_fn = my_collate)\n",
    "dataiter = next(iter(train_dataloader))\n",
    "dataiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "295908e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class MultilabelModel(torch.nn.Module):\n",
    "#\n",
    "#    def __init__(self, input_dim, output_dim):\n",
    "#        super(MultilabelModel, self).__init__()\n",
    "#\n",
    "#        self.linear1 = torch.nn.Linear(input_dim, 128)\n",
    "#        self.Sigmoid = torch.nn.Sigmoid()\n",
    "#        self.linear2 = torch.nn.Linear(128, output_dim)\n",
    "#\n",
    "#    def forward(self, x):\n",
    "#        \n",
    "#        output = self.linear1(x)\n",
    "#        output = self.linear2(output)\n",
    "#        output = self.Sigmoid(output)\n",
    "#        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f34a66f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5010, 0.4903, 0.4927,  ..., 0.5146, 0.5108, 0.4875],\n",
      "        [0.4982, 0.4926, 0.4963,  ..., 0.5096, 0.5105, 0.4882],\n",
      "        [0.4962, 0.4942, 0.4951,  ..., 0.5062, 0.5076, 0.4869],\n",
      "        ...,\n",
      "        [0.5000, 0.4911, 0.4916,  ..., 0.5132, 0.5087, 0.4865],\n",
      "        [0.4930, 0.4922, 0.4956,  ..., 0.5104, 0.5096, 0.4833],\n",
      "        [0.5013, 0.4897, 0.4945,  ..., 0.5062, 0.5109, 0.4850]],\n",
      "       grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#clf = MultilabelModel(26146, output_dim)\n",
    "#print(clf(dataiter['x']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "3e30da6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, embed_dim, hidden_dim, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, 101)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = x.reshape([x.shape[0],1,x.shape[1]])\n",
    "        h0 = torch.zeros(self.num_layers, out.size(0), self.hidden_dim)\n",
    "        c0 = torch.zeros(self.num_layers, out.size(0), self.hidden_dim)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, (hn, cn) = self.lstm(out, (h0, c0))  \n",
    "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "5d32db0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.4964, 0.4776,  ..., 0.5048, 0.5088, 0.5486],\n",
      "        [0.4721, 0.5013, 0.5054,  ..., 0.5076, 0.5221, 0.5306],\n",
      "        [0.5013, 0.4870, 0.4906,  ..., 0.5229, 0.4880, 0.5525],\n",
      "        ...,\n",
      "        [0.5169, 0.4927, 0.5296,  ..., 0.5313, 0.4994, 0.5138],\n",
      "        [0.5356, 0.4562, 0.4910,  ..., 0.5017, 0.5111, 0.5173],\n",
      "        [0.5174, 0.4782, 0.5048,  ..., 0.5377, 0.4981, 0.5026]],\n",
      "       grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "clf = LSTM(26146, 64, 128, 1)\n",
    "print(clf(dataiter['x']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f2b43a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3h/x2xrdjwn22bbdw5d98hd13w80000gn/T/ipykernel_24878/2406105316.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mn_total_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m#batch = batch.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/3h/x2xrdjwn22bbdw5d98hd13w80000gn/T/ipykernel_24878/626078167.py\u001b[0m in \u001b[0;36mmy_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# loss and optimizer\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(clf.parameters(),lr=learning_rate)\n",
    "\n",
    "# training loop\n",
    "n_total_steps = len(train_dataloader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        #batch = batch.to(device)\n",
    "\n",
    "        # forward\n",
    "        outputs = clf(batch['x'])\n",
    "        loss = criterion(outputs, batch['target'])\n",
    "        \n",
    "        predictions = np.where(outputs.detach().numpy()>=0.5, 1, 0)\n",
    "        f1_acc = f1_score(batch['target'].detach().numpy(), predictions, average=\"samples\", zero_division=1)\n",
    "\n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #if (i + 1) % 100 == 0:\n",
    "    print(f'epoch {epoch + 1} / {num_epochs}, loss = {loss.item():.4f}, training f1 score = {f1_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "b4d3c3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "test_preds = []\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "        outputs = clf(batch['x'])\n",
    "        \n",
    "        predictions = np.where(outputs.detach().numpy()>=0.5, 1, 0)\n",
    "        test_preds.append(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "fe9b940c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_preds[1][39])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "8b2cb51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final_result = []\n",
    "for i in range(len(test_preds)):\n",
    "    for j in range(4):\n",
    "        test_final_result.append(test_preds[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "ebae9abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1667: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = value\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    }
   ],
   "source": [
    "NN_result = test_df[['identifier']]\n",
    "NN_result.loc[:,'Predict'] = ''\n",
    "final_result = []\n",
    "for i in range(len(test_preds)):\n",
    "    for j in range(batch_size):\n",
    "        final_result.append(test_preds[i][j])\n",
    "print(len(final_result))\n",
    "for i in range(len(final_result)):\n",
    "    result = final_result[i]\n",
    "    if result[-1] == 1 or len(find(list(result), 1)) == 0:\n",
    "        NN_result.loc[i,'Predict'] = -1\n",
    "    else:\n",
    "        NN_result.loc[i,'Predict'] = ' '.join(str(e) for e in find(list(result), 1))\n",
    "        \n",
    "NN_result = NN_result.rename(columns={'identifier':'ID'})\n",
    "NN_result.to_csv('./NN_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "80cd5bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>795</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>796</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>797</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>798</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>799</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID Predict\n",
       "0      0      -1\n",
       "1      1      -1\n",
       "2      2      -1\n",
       "3      3      -1\n",
       "4      4      -1\n",
       "..   ...     ...\n",
       "795  795      54\n",
       "796  796      -1\n",
       "797  797      -1\n",
       "798  798      -1\n",
       "799  799      -1\n",
       "\n",
       "[800 rows x 2 columns]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf63636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
