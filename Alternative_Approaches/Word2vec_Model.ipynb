{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1688962f",
   "metadata": {},
   "source": [
    "# Alternative Approaches - Word Embedding\n",
    "This notebook displays our alternative approaches of neural networks using word2vec on title and abstract. There are two kinds of neural networks: traditional networks and LSTMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86e90b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a41e5094",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prolific = 99\n",
    "n_text = 4999\n",
    "n_authors = 21245\n",
    "n_prolific = 100\n",
    "n_coauthors = n_authors - n_prolific + 1\n",
    "n_years = 19\n",
    "n_venues = 466\n",
    "batch_size = 40\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dcc8249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fdc62f",
   "metadata": {},
   "source": [
    "# Part 1: Data Prepossessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "835927d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25793, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_path = '/Users/sicenxi/Documents/GitHub/COMP90051_Project2_Group9/data/train.json'\n",
    "test_data_path = '/Users/sicenxi/Documents/GitHub/COMP90051_Project2_Group9/data/test.json'\n",
    "# read train json file\n",
    "with open(train_data_path, 'r', encoding='utf-8') as f:\n",
    "    raw_train = json.load(f)\n",
    "# read test json file\n",
    "with open(test_data_path, 'r', encoding='utf-8') as f:\n",
    "    raw_test = json.load(f)\n",
    "    \n",
    "# extract coauthors as a new key from train.json\n",
    "title_list = []\n",
    "abstract_list = []\n",
    "word_list = []\n",
    "for i in range(len(raw_train)):\n",
    "    coauthors = []\n",
    "    prolific_authors = []\n",
    "    for auth in raw_train[i]['authors']:\n",
    "        if auth >= max_prolific:\n",
    "            coauthors.append(auth)\n",
    "        else:\n",
    "            prolific_authors.append(auth)\n",
    "    if len(prolific_authors) == 0:\n",
    "        prolific_authors = -1\n",
    "    raw_train[i]['coauthors'] = coauthors\n",
    "    raw_train[i]['prolific_authors'] = prolific_authors\n",
    "    \n",
    "train_df = pd.DataFrame.from_dict(raw_train)\n",
    "train_df = train_df.drop(['authors'], axis=1)\n",
    "train_df['venue'] = train_df['venue'].replace('', 465)\n",
    "for i in range(len(train_df)):\n",
    "    title_list.append(train_df['title'][i])\n",
    "    abstract_list.append(train_df['abstract'][i])\n",
    "    word_list.append(train_df['title'][i])\n",
    "    word_list.append(train_df['abstract'][i])\n",
    "\n",
    "    \n",
    "test_df = pd.DataFrame.from_dict(raw_test)\n",
    "test_df['venue'] = test_df['venue'].replace('', 465)\n",
    "for i in range(len(test_df)):\n",
    "    title_list.append(test_df['title'][i])\n",
    "    abstract_list.append(test_df['abstract'][i])\n",
    "    word_list.append(test_df['title'][i])\n",
    "    word_list.append(test_df['abstract'][i])\n",
    "\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "022d83a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>abstract</th>\n",
       "      <th>venue</th>\n",
       "      <th>title</th>\n",
       "      <th>coauthors</th>\n",
       "      <th>prolific_authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>[2455, 1858, 2335, 1543, 1800, 1860, 2000, 286...</td>\n",
       "      <td>20</td>\n",
       "      <td>[41, 1550, 1563, 1594, 1544, 1919, 1644, 37, 1...</td>\n",
       "      <td>[13720]</td>\n",
       "      <td>[42, 36]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>[40, 1542, 1691, 2449, 1535, 3616, 2206, 1904,...</td>\n",
       "      <td>2</td>\n",
       "      <td>[1731, 47, 11, 57, 4624, 1525, 1535, 47, 11, 3...</td>\n",
       "      <td>[1359, 15881]</td>\n",
       "      <td>[45]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>[46, 1624, 1547, 56, 1687, 1644, 6, 7, 3386, 1...</td>\n",
       "      <td>4</td>\n",
       "      <td>[40, 1733, 1735, 1540, 1655, 46, 1624, 1547, 5...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[97]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year                                           abstract  venue  \\\n",
       "0     9  [2455, 1858, 2335, 1543, 1800, 1860, 2000, 286...     20   \n",
       "1    15  [40, 1542, 1691, 2449, 1535, 3616, 2206, 1904,...      2   \n",
       "2    10  [46, 1624, 1547, 56, 1687, 1644, 6, 7, 3386, 1...      4   \n",
       "\n",
       "                                               title      coauthors  \\\n",
       "0  [41, 1550, 1563, 1594, 1544, 1919, 1644, 37, 1...        [13720]   \n",
       "1  [1731, 47, 11, 57, 4624, 1525, 1535, 47, 11, 3...  [1359, 15881]   \n",
       "2  [40, 1733, 1735, 1540, 1655, 46, 1624, 1547, 5...             []   \n",
       "\n",
       "  prolific_authors  \n",
       "0         [42, 36]  \n",
       "1             [45]  \n",
       "2             [97]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_prolific = train_df[train_df['prolific_authors'] != -1]\n",
    "train_df_noprolific = train_df[train_df['prolific_authors'] == -1]\n",
    "train_df_combine = pd.concat([train_df_prolific, train_df_noprolific.tail(7000)], axis=0)\n",
    "train_df_combine = train_df_combine.reset_index(drop=True)\n",
    "train_df_combine.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "758c2d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2vec\n",
    "titlemodel = Word2Vec(title_list, min_count=1, vector_size=128)\n",
    "titleword_vectors = titlemodel.wv\n",
    "titlekeyword_list = []\n",
    "for i in range(len(train_df)):\n",
    "    word_vec = np.zeros(128)\n",
    "    for title in title_list[i]:\n",
    "        word_vec += titleword_vectors[title]\n",
    "    titlekeyword_list.append(word_vec/len(title_list[i]))\n",
    "titlekeyword_list = np.array(titlekeyword_list)\n",
    "\n",
    "abstractmodel = Word2Vec(abstract_list, min_count=1, vector_size=128)\n",
    "abstractword_vectors = abstractmodel.wv\n",
    "abstractkeyword_list = []\n",
    "for i in range(len(train_df)):\n",
    "    word_vec = np.zeros(128)\n",
    "    for abstract in abstract_list[i]:\n",
    "        word_vec += abstractword_vectors[abstract]\n",
    "    abstractkeyword_list.append(word_vec/len(abstract_list[i]))\n",
    "abstractkeyword_list = np.array(abstractkeyword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8534a07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuthorDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, istrain):\n",
    "        self.data = dataframe\n",
    "        self.x = dataframe[['year', 'venue', 'coauthors', 'abstract', 'title']]\n",
    "        self.istrain = istrain\n",
    "        if self.istrain == True:\n",
    "            self.y = self.data.prolific_authors\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        year = self.data.year[index]\n",
    "        venue = self.data.venue[index]\n",
    "        title = self.data.title[index]\n",
    "        abstract = self.data.abstract[index]\n",
    "        \n",
    "        # coauthors to one hot\n",
    "        coauthors = self.data.coauthors[index]\n",
    "        coauthor_list = [0] * (n_authors - n_prolific + 2) # 21147 elements with the last element for empty coauthors\n",
    "        if coauthors == []:\n",
    "            coauthor_list[-1] = 1\n",
    "        else:\n",
    "            for coauthor in coauthors:\n",
    "                coauthor_list[coauthor-n_prolific] = 1\n",
    "                \n",
    "        x_output = {\"title\": title, \"abstract\": abstract, \"year\": year, \"venue\": venue, \"coauthors\": coauthor_list}\n",
    "        \n",
    "        # target to one hot\n",
    "        if self.istrain == True:\n",
    "            prolific_list = [0] * (n_prolific)\n",
    "            if self.y[index] != -1:\n",
    "                for prolific in self.y[index]:\n",
    "                    prolific_list[prolific] = 1\n",
    "            y_output = prolific_list\n",
    "            return x_output, y_output\n",
    "        else:\n",
    "            return x_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74030042",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = AuthorDataset(train_df, istrain = True)\n",
    "testing_df = AuthorDataset(test_df, istrain = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c756e39",
   "metadata": {},
   "source": [
    "# Part 2: Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0425b260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    # for training set\n",
    "    if len(batch[0]) == 2:\n",
    "        output = {\"title\": [], \"abstract\": [], \"year\": [], \"venue\": [], \"coauthors\": [], \"target\": []}\n",
    "        \n",
    "        for data in batch:\n",
    "            x = data[0]\n",
    "            target = data[1]\n",
    "            \n",
    "            output['title'] += [torch.tensor(x['title'], dtype=torch.long)]\n",
    "            output['abstract'] += [torch.tensor(x['abstract'], dtype=torch.long)]\n",
    "            output['year'] += [x['year']]\n",
    "            output['venue'] += [x['venue']]\n",
    "            output['coauthors'] += [torch.tensor(x['coauthors'], dtype=torch.long)]\n",
    "            output['target'] += [target]\n",
    "            \n",
    "        output['year'] = torch.tensor(output['year'], dtype=torch.long)\n",
    "        output['venue'] = torch.tensor(output['venue'], dtype=torch.long)\n",
    "        output['target'] = torch.tensor(output['target'], dtype=torch.float)\n",
    "        return output\n",
    "    \n",
    "    # for testing set\n",
    "    else:\n",
    "        output = {\"title\": [], \"abstract\": [], \"year\": [], \"venue\": [], \"coauthors\": []}\n",
    "        \n",
    "        for data in batch:\n",
    "            output['title'] += [torch.tensor(data['title'], dtype=torch.long)]\n",
    "            output['abstract'] += [torch.tensor(data['abstract'], dtype=torch.long)]\n",
    "            output['year'] += [data['year']]\n",
    "            output['venue'] += [data['venue']]\n",
    "            output['coauthors'] += [torch.tensor(data['coauthors'], dtype=torch.long)]\n",
    "            \n",
    "        output['year'] = torch.tensor(output['year'], dtype=torch.long)\n",
    "        output['venue'] = torch.tensor(output['venue'], dtype=torch.long)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0a5e9fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset = training_df, batch_size = batch_size, shuffle=True, collate_fn = my_collate)\n",
    "test_dataloader = DataLoader(dataset = testing_df, batch_size = batch_size, shuffle=False, collate_fn = my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "860c9131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We combine the traditional neural networks and LSTMs into one cell.\n",
    "# We uncomment the codes when switching to another approach.\n",
    "\n",
    "class Multilabel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, embed_dim, hidden_dim, num_layers):\n",
    "        super(Multilabel, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.weights = torch.FloatTensor(titlekeyword_list)\n",
    "        #self.weights = torch.FloatTensor(abstractkeyword_list)\n",
    "        self.embedding = nn.Embedding.from_pretrained(self.weights)\n",
    "        self.embedding.requires_grad = False\n",
    "        self.embedding = nn.Embedding(n_text+1, embed_dim)\n",
    "        \n",
    "        self.linear1 = nn.Linear(input_size, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, 100)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        #self.lstm = nn.LSTM(input_size, hidden_dim, num_layers, batch_first=True)\n",
    "        #self.fc = nn.Linear(hidden_dim, 100)\n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        word2vec_title_list = []\n",
    "        for curr_title in x['title']:\n",
    "            title_vec = self.embedding(curr_title)\n",
    "            word2vec_title_list.append(title_vec.mean(dim = 0))\n",
    "        embed_title = torch.stack(word2vec_title_list)\n",
    "        \n",
    "        word2vec_abstract_list = []\n",
    "        for curr_abstract in x['abstract']:\n",
    "            abstract_vec = self.embedding(curr_abstract)\n",
    "            word2vec_abstract_list.append(abstract_vec.mean(dim = 0))\n",
    "        embed_abstract = torch.stack(word2vec_abstract_list)\n",
    "        \n",
    "        #out = embed_title.reshape([embed_title.shape[0], 1, embed_title.shape[1]])\n",
    "        \n",
    "        #out = embed_title.reshape([embed_abstract.shape[0], 1, embed_abstract.shape[1]])\n",
    "        \n",
    "        \n",
    "        \n",
    "        #h0 = torch.zeros(self.num_layers, out.size(0), self.hidden_dim)\n",
    "        #c0 = torch.zeros(self.num_layers, out.size(0), self.hidden_dim)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        #out, (hn, cn) = self.lstm(out, (h0, c0))  \n",
    "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        #out = self.fc(out[:, -1, :]) \n",
    "        #out = self.sigmoid(out)\n",
    "        #return out\n",
    "        \n",
    "        out = self.linear1(embed_title)\n",
    "        #out = self.linear1(embed_abstract)\n",
    "        out = self.activation(out)\n",
    "        out = self.linear2(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05ebbf01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = Multilabel(64, 64, 128, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b1fb5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(clf.parameters(),lr=0.1)\n",
    "\n",
    "# training loop\n",
    "n_total_steps = len(train_dataloader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        #batch = batch.to(device)\n",
    "\n",
    "        # forward\n",
    "        outputs = clf(batch)\n",
    "        \n",
    "        loss = criterion(outputs, batch['target'])\n",
    "        \n",
    "        predictions = np.where(outputs.detach().numpy()>=0.5, 1, 0)\n",
    "        f1_acc = f1_score(batch['target'].detach().numpy(), predictions, average=\"samples\", zero_division=1)\n",
    "\n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'epoch {epoch + 1} / {num_epochs}, loss = {loss.item():.4f}, training f1 score = {f1_acc:.4f}')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a579e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "test_preds = []\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "        outputs = clf(batch)\n",
    "        \n",
    "        predictions = np.where(outputs.detach().numpy()>0.5, 1, 0)\n",
    "        test_preds.append(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6dee4881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find(lst, num):\n",
    "    result = []\n",
    "    for i, x in enumerate(lst):\n",
    "        if x==num:\n",
    "            result.append(i)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f65af5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1667: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = value\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    }
   ],
   "source": [
    "NN_result = test_df[['identifier']]\n",
    "NN_result.loc[:,'Predict'] = ''\n",
    "final_result = []\n",
    "for i in range(len(test_preds)):\n",
    "    for j in range(batch_size):\n",
    "        final_result.append(test_preds[i][j])\n",
    "print(len(final_result))\n",
    "for i in range(len(final_result)):\n",
    "    result = final_result[i]\n",
    "    if len(find(list(result), 1)) == 0:\n",
    "        NN_result.loc[i,'Predict'] = -1\n",
    "    else:\n",
    "        NN_result.loc[i,'Predict'] = ' '.join(str(e) for e in find(list(result), 1))\n",
    "        \n",
    "NN_result = NN_result.rename(columns={'identifier':'ID'})\n",
    "NN_result.to_csv('./NN_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318970df",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81d34e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
